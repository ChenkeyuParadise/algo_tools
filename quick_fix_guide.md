# çˆ¬è™«åŠŸèƒ½å¿«é€Ÿä¿®å¤å’ŒéªŒè¯æŒ‡å—

## ğŸš€ å¿«é€Ÿå¯åŠ¨æ­¥éª¤

### 1. ç¯å¢ƒå‡†å¤‡ï¼ˆå¿…éœ€ï¼‰
```bash
# æ–¹æ³•ä¸€ï¼šåˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv crawler_env
source crawler_env/bin/activate  # Linux/Mac
# æˆ–è€…
crawler_env\Scripts\activate     # Windows

# æ–¹æ³•äºŒï¼šä½¿ç”¨condaï¼ˆå¦‚æœæœ‰ï¼‰
conda create -n crawler python=3.9
conda activate crawler

# å®‰è£…ä¾èµ–
pip install requests beautifulsoup4 lxml fake-useragent flask pandas matplotlib seaborn plotly schedule APScheduler
```

### 2. å¿«é€ŸéªŒè¯è„šæœ¬
åˆ›å»º `quick_test.py`ï¼š
```python
#!/usr/bin/env python3
"""å¿«é€ŸéªŒè¯çˆ¬è™«åŠŸèƒ½"""

import sys
sys.path.append('.')

def quick_test():
    try:
        print("ğŸ”§ å¯¼å…¥æ¨¡å—...")
        from keyword_crawler.crawler import SearchCrawler
        from keyword_crawler.database import DatabaseManager
        
        print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        print("ğŸ”§ åˆå§‹åŒ–çˆ¬è™«...")
        db_manager = DatabaseManager()
        crawler = SearchCrawler(db_manager)
        
        print("âœ… çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
        
        print("ğŸ”§ æµ‹è¯•æœç´¢å¼•æ“çŠ¶æ€...")
        test_results = crawler.test_all_engines()
        
        success_count = 0
        for engine, result in test_results.items():
            if result['success']:
                print(f"âœ… {engine}: {result['result_count']} ä¸ªç»“æœ")
                success_count += 1
            else:
                print(f"âŒ {engine}: {result['error']}")
        
        print(f"\nğŸ“Š æ€»ç»“: {success_count}/{len(test_results)} ä¸ªæœç´¢å¼•æ“å¯ç”¨")
        
        if success_count > 0:
            print("ğŸ‰ çˆ¬è™«åŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨ï¼")
            return True
        else:
            print("âš ï¸  æ‰€æœ‰æœç´¢å¼•æ“éƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–åçˆ¬è™«é™åˆ¶")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    quick_test()
```

### 3. è¿è¡Œå¿«é€Ÿæµ‹è¯•
```bash
python quick_test.py
```

## ğŸ”§ å¸¸è§é—®é¢˜è§£å†³

### é—®é¢˜1ï¼šå¯¼å…¥é”™è¯¯
```
ModuleNotFoundError: No module named 'requests'
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# ç¡®ä¿åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­
pip install requests beautifulsoup4 lxml

# æ£€æŸ¥å®‰è£…
python -c "import requests; print('requests OK')"
python -c "import bs4; print('beautifulsoup4 OK')"
```

### é—®é¢˜2ï¼šfake_useragenté”™è¯¯
```
FakeUserAgentError: Maximum amount of retries reached
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# çˆ¬è™«å·²ä¼˜åŒ–ï¼Œä¼šè‡ªåŠ¨é™çº§åˆ°å†…ç½®User-Agentæ± 
# æ— éœ€é¢å¤–å¤„ç†ï¼Œæˆ–è€…å¯ä»¥æ‰‹åŠ¨å®‰è£…ï¼š
pip install fake-useragent
```

### é—®é¢˜3ï¼šæœç´¢ç»“æœä¸ºç©º
```
âœ… baidu: 0 ä¸ªç»“æœ
```

**å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆ**ï¼š
1. **åçˆ¬è™«æ‹¦æˆª**ï¼šå¢åŠ å»¶è¿Ÿ
   ```python
   from keyword_crawler.config import Config
   Config.REQUEST_DELAY = 5  # å¢åŠ åˆ°5ç§’
   ```

2. **CSSé€‰æ‹©å™¨å¤±æ•ˆ**ï¼šæ£€æŸ¥æ—¥å¿—
   ```bash
   tail -f logs/crawler.log
   ```

3. **ç½‘ç»œé—®é¢˜**ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥
   ```bash
   curl -I https://www.baidu.com
   ```

### é—®é¢˜4ï¼šéªŒè¯ç æ‹¦æˆª
```
è¯·æ±‚è¢«æ‹¦æˆª: éªŒè¯ç  captcha
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **å¢åŠ å»¶è¿Ÿ**ï¼š
   ```python
   Config.REQUEST_DELAY = 10  # 10ç§’å»¶è¿Ÿ
   ```

2. **ä½¿ç”¨ä»£ç†**ï¼ˆé«˜çº§ï¼‰ï¼š
   ```python
   # åœ¨crawler.pyä¸­æ·»åŠ ä»£ç†æ”¯æŒ
   proxies = {
       'http': 'http://proxy:port',
       'https': 'https://proxy:port'
   }
   response = self.session.get(url, proxies=proxies)
   ```

## ğŸ“‹ åŠŸèƒ½éªŒè¯æ¸…å•

### âœ… åŸºç¡€åŠŸèƒ½éªŒè¯
- [ ] æ¨¡å—å¯¼å…¥æˆåŠŸ
- [ ] æ•°æ®åº“åˆå§‹åŒ–
- [ ] çˆ¬è™«åˆå§‹åŒ–
- [ ] æœç´¢å¼•æ“çŠ¶æ€æ£€æµ‹
- [ ] è‡³å°‘ä¸€ä¸ªæœç´¢å¼•æ“å¯ç”¨

### âœ… æ ¸å¿ƒåŠŸèƒ½éªŒè¯
```python
# å•å…³é”®è¯æœç´¢
results = crawler.search_keyword("Python", "baidu", pages=1)
print(f"æœç´¢ç»“æœ: {len(results)} ä¸ª")

# å¤šå…³é”®è¯æœç´¢
keywords = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ "]
all_results = crawler.search_all_engines(keywords, pages=1)
print(f"æ€»ç»“æœæ•°: {sum(len(r) for kw in all_results.values() for r in kw.values())}")
```

### âœ… Webç•Œé¢éªŒè¯
```bash
# å¯åŠ¨Webç•Œé¢
python main.py web

# è®¿é—® http://localhost:5000
# æ£€æŸ¥å„é¡µé¢åŠŸèƒ½ï¼š
# - é¦–é¡µç»Ÿè®¡
# - å…³é”®è¯ç®¡ç†
# - ä»»åŠ¡ç®¡ç†
# - æ•°æ®å¯è§†åŒ–
```

## ğŸš¨ æ•…éšœæ’é™¤

### 1. å®Œå…¨æ— æ³•æœç´¢
**ç—‡çŠ¶**ï¼šæ‰€æœ‰æœç´¢å¼•æ“éƒ½è¿”å›0ç»“æœ

**æ£€æŸ¥æ­¥éª¤**ï¼š
```python
# æ£€æŸ¥ç½‘ç»œè¿æ¥
import requests
try:
    response = requests.get("https://www.baidu.com", timeout=10)
    print(f"ç½‘ç»œè¿æ¥æ­£å¸¸: {response.status_code}")
except Exception as e:
    print(f"ç½‘ç»œè¿æ¥å¤±è´¥: {e}")

# æ£€æŸ¥User-Agent
crawler = SearchCrawler()
ua = crawler._get_random_user_agent()
print(f"User-Agent: {ua}")

# æ£€æŸ¥é€‰æ‹©å™¨
from keyword_crawler.crawler import UPDATED_SELECTORS
print("ç™¾åº¦é€‰æ‹©å™¨:", UPDATED_SELECTORS['baidu']['result'])
```

### 2. éƒ¨åˆ†æœç´¢å¼•æ“å¤±æ•ˆ
**ç—‡çŠ¶**ï¼šåªæœ‰æŸäº›æœç´¢å¼•æ“ä¸å·¥ä½œ

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# å•ç‹¬æµ‹è¯•é—®é¢˜å¼•æ“
result = crawler.test_search_engine("baidu")
print(f"ç™¾åº¦æµ‹è¯•ç»“æœ: {result}")

# æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯
if result['error']:
    print(f"é”™è¯¯è¯¦æƒ…: {result['error']}")
```

### 3. æ€§èƒ½é—®é¢˜
**ç—‡çŠ¶**ï¼šæœç´¢é€Ÿåº¦å¾ˆæ…¢

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š
```python
# è°ƒæ•´å¹¶å‘ï¼ˆå¦‚æœä½¿ç”¨å¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰
Config.MAX_WORKERS = 2  # å‡å°‘å¹¶å‘

# å‡å°‘é¡µæ•°
results = crawler.search_keyword("å…³é”®è¯", "baidu", pages=1)  # åªæœç´¢1é¡µ

# ä¼˜åŒ–å»¶è¿Ÿ
Config.REQUEST_DELAY = 3  # å¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. ç”Ÿäº§ç¯å¢ƒé…ç½®
```python
# config_production.py
class ProductionConfig:
    REQUEST_DELAY = 5      # è¾ƒé•¿å»¶è¿Ÿç¡®ä¿ç¨³å®š
    MAX_RETRIES = 5        # æ›´å¤šé‡è¯•
    REQUEST_TIMEOUT = 30   # æ›´é•¿è¶…æ—¶
    
    # å¯ç”¨è¯¦ç»†æ—¥å¿—
    DEBUG = False
    LOG_LEVEL = "INFO"
```

### 2. ç›‘æ§è„šæœ¬
```python
# monitor.py
import time
from datetime import datetime

def monitor_crawler():
    crawler = SearchCrawler()
    
    while True:
        try:
            # æµ‹è¯•æ‰€æœ‰å¼•æ“
            results = crawler.test_all_engines()
            success_count = sum(1 for r in results.values() if r['success'])
            
            print(f"[{datetime.now()}] {success_count}/{len(results)} å¼•æ“å¯ç”¨")
            
            if success_count == 0:
                print("âš ï¸  æ‰€æœ‰å¼•æ“éƒ½ä¸å¯ç”¨ï¼Œå‘é€å‘Šè­¦")
                # å‘é€é‚®ä»¶/çŸ­ä¿¡å‘Šè­¦
                
        except Exception as e:
            print(f"ç›‘æ§å¼‚å¸¸: {e}")
            
        time.sleep(300)  # 5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

if __name__ == "__main__":
    monitor_crawler()
```

### 3. æ•°æ®å¤‡ä»½
```bash
# å®šæœŸå¤‡ä»½æ•°æ®åº“
cp data/crawler_data.db data/backup_$(date +%Y%m%d).db

# å¤‡ä»½ç»“æœæ–‡ä»¶
tar -czf results_backup_$(date +%Y%m%d).tar.gz results/
```

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### æµ‹è¯•è„šæœ¬
```python
# benchmark.py
import time
from keyword_crawler.crawler import SearchCrawler

def benchmark_test():
    crawler = SearchCrawler()
    test_keywords = ["Python", "Java", "JavaScript"]
    
    start_time = time.time()
    
    for keyword in test_keywords:
        print(f"æœç´¢å…³é”®è¯: {keyword}")
        results = crawler.search_keyword(keyword, "baidu", pages=1)
        print(f"ç»“æœæ•°: {len(results)}")
    
    total_time = time.time() - start_time
    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"å¹³å‡æ¯ä¸ªå…³é”®è¯: {total_time/len(test_keywords):.2f}ç§’")

if __name__ == "__main__":
    benchmark_test()
```

### é¢„æœŸæ€§èƒ½æŒ‡æ ‡
```
âœ… æ­£å¸¸ç¯å¢ƒä¸‹çš„æ€§èƒ½åŸºå‡†:
- å•å…³é”®è¯æœç´¢: 3-8ç§’
- æ¯é¡µç»“æœæ•°: 8-15ä¸ª
- æˆåŠŸç‡: 80-95%
- å†…å­˜ä½¿ç”¨: <100MB
- CPUä½¿ç”¨: <10%
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### è°ƒè¯•æ¨¡å¼
```python
# å¯ç”¨è¯¦ç»†è°ƒè¯•
import logging
logging.getLogger('SearchCrawler').setLevel(logging.DEBUG)

# æˆ–åœ¨å‘½ä»¤è¡Œä¸­
python main.py crawl -k "æµ‹è¯•" --debug
```

### æ—¥å¿—åˆ†æ
```bash
# æŸ¥çœ‹æœ€è¿‘çš„é”™è¯¯
grep "ERROR" logs/crawler.log | tail -10

# ç»Ÿè®¡æˆåŠŸç‡
grep "æœç´¢å®Œæˆ" logs/crawler.log | wc -l
grep "æœç´¢å¤±è´¥" logs/crawler.log | wc -l

# æŸ¥çœ‹è¢«æ‹¦æˆªçš„è¯·æ±‚
grep "è¢«æ‹¦æˆª" logs/crawler.log
```

---

**âš ï¸ é‡è¦æé†’**ï¼š
1. è¯·éµå®ˆæœç´¢å¼•æ“çš„æœåŠ¡æ¡æ¬¾
2. åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›
3. å®šæœŸæ›´æ–°é€‰æ‹©å™¨ä»¥é€‚åº”é¡µé¢å˜åŒ–
4. åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ç”¨å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œç›‘æ§