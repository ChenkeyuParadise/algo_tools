#!/usr/bin/env python3
"""
çˆ¬è™«åŠŸèƒ½æµ‹è¯•è„šæœ¬

ç”¨äºæµ‹è¯•SearchCrawlerçš„å„é¡¹åŠŸèƒ½
"""

import sys
import os
import logging
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

def setup_test_logging():
    """è®¾ç½®æµ‹è¯•æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('test_crawler.log', encoding='utf-8')
        ]
    )

def test_imports():
    """æµ‹è¯•å¯¼å…¥åŠŸèƒ½"""
    print("ğŸ”§ æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    
    try:
        from keyword_crawler.crawler import SearchCrawler
        from keyword_crawler.database import DatabaseManager
        from keyword_crawler.config import Config
        print("âœ… æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_dependencies():
    """æµ‹è¯•ä¾èµ–åŒ…"""
    print("\nğŸ”§ æµ‹è¯•ä¾èµ–åŒ…...")
    
    dependencies = {
        'requests': False,
        'beautifulsoup4': False,
        'lxml': False,
        'fake_useragent': False
    }
    
    try:
        import requests
        dependencies['requests'] = True
        print("âœ… requests å¯ç”¨")
    except ImportError:
        print("âŒ requests ä¸å¯ç”¨")
    
    try:
        from bs4 import BeautifulSoup
        dependencies['beautifulsoup4'] = True
        print("âœ… beautifulsoup4 å¯ç”¨")
    except ImportError:
        print("âŒ beautifulsoup4 ä¸å¯ç”¨")
    
    try:
        import lxml
        dependencies['lxml'] = True
        print("âœ… lxml å¯ç”¨")
    except ImportError:
        print("âŒ lxml ä¸å¯ç”¨")
    
    try:
        from fake_useragent import UserAgent
        dependencies['fake_useragent'] = True
        print("âœ… fake_useragent å¯ç”¨")
    except ImportError:
        print("âŒ fake_useragent ä¸å¯ç”¨ (å¯é€‰ä¾èµ–)")
    
    essential_deps = ['requests', 'beautifulsoup4']
    missing_essential = [dep for dep in essential_deps if not dependencies[dep]]
    
    if missing_essential:
        print(f"âŒ ç¼ºå°‘å¿…è¦ä¾èµ–: {missing_essential}")
        return False
    else:
        print("âœ… å¿…è¦ä¾èµ–æ£€æŸ¥é€šè¿‡")
        return True

def test_crawler_initialization():
    """æµ‹è¯•çˆ¬è™«åˆå§‹åŒ–"""
    print("\nğŸ”§ æµ‹è¯•çˆ¬è™«åˆå§‹åŒ–...")
    
    try:
        from keyword_crawler.crawler import SearchCrawler
        from keyword_crawler.database import DatabaseManager
        
        db_manager = DatabaseManager()
        crawler = SearchCrawler(db_manager)
        
        print("âœ… çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
        print(f"âœ… æ—¥å¿—å™¨è®¾ç½®: {type(crawler.logger).__name__}")
        print(f"âœ… ä¼šè¯è®¾ç½®: {type(crawler.session).__name__}")
        
        # æµ‹è¯•User-Agentç”Ÿæˆ
        ua = crawler._get_random_user_agent()
        print(f"âœ… User-Agentç”Ÿæˆ: {ua[:50]}...")
        
        return crawler
        
    except Exception as e:
        print(f"âŒ çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
        traceback.print_exc()
        return None

def test_url_building():
    """æµ‹è¯•URLæ„å»º"""
    print("\nğŸ”§ æµ‹è¯•URLæ„å»º...")
    
    try:
        from keyword_crawler.config import Config
        from urllib.parse import quote
        
        test_keyword = "Pythonç¼–ç¨‹"
        engines = Config.get_enabled_search_engines()
        
        for engine_name, engine_config in engines.items():
            search_url = engine_config['search_url'].format(
                keyword=quote(test_keyword),
                page=0
            )
            print(f"âœ… {engine_config['name']}: {search_url}")
        
        return True
        
    except Exception as e:
        print(f"âŒ URLæ„å»ºå¤±è´¥: {e}")
        return False

def test_selectors():
    """æµ‹è¯•CSSé€‰æ‹©å™¨"""
    print("\nğŸ”§ æµ‹è¯•CSSé€‰æ‹©å™¨...")
    
    try:
        from keyword_crawler.crawler import UPDATED_SELECTORS
        
        for engine, selectors in UPDATED_SELECTORS.items():
            print(f"ğŸ“‹ {engine.upper()} é€‰æ‹©å™¨:")
            print(f"  ç»“æœå®¹å™¨: {selectors['result']}")
            print(f"  æ ‡é¢˜: {selectors['title']}")
            print(f"  é“¾æ¥: {selectors['url']}")
            print(f"  æ‘˜è¦: {selectors['snippet']}")
            print()
        
        return True
        
    except Exception as e:
        print(f"âŒ é€‰æ‹©å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_search_single_keyword(crawler):
    """æµ‹è¯•å•å…³é”®è¯æœç´¢"""
    print("\nğŸ”§ æµ‹è¯•å•å…³é”®è¯æœç´¢...")
    
    if not crawler:
        print("âŒ çˆ¬è™«æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    test_keyword = "Python"
    test_engines = ["baidu", "bing"]  # é™åˆ¶æµ‹è¯•èŒƒå›´
    
    results = {}
    
    for engine in test_engines:
        print(f"\nğŸ“ æµ‹è¯•æœç´¢å¼•æ“: {engine}")
        
        try:
            # åªæœç´¢1é¡µï¼Œå‡å°‘æµ‹è¯•æ—¶é—´
            search_results = crawler.search_keyword(test_keyword, engine, pages=1)
            results[engine] = search_results
            
            print(f"âœ… {engine} æœç´¢æˆåŠŸï¼Œè·å– {len(search_results)} ä¸ªç»“æœ")
            
            # æ˜¾ç¤ºå‰3ä¸ªç»“æœæ ·ä¾‹
            for i, result in enumerate(search_results[:3]):
                print(f"  [{i+1}] {result['title'][:50]}...")
                if result['url']:
                    print(f"      {result['url'][:60]}...")
                
        except Exception as e:
            print(f"âŒ {engine} æœç´¢å¤±è´¥: {e}")
            results[engine] = []
    
    total_results = sum(len(r) for r in results.values())
    print(f"\nğŸ“Š æœç´¢ç»“æœæ±‡æ€»: å…± {total_results} ä¸ªç»“æœ")
    
    return total_results > 0

def test_all_engines(crawler):
    """æµ‹è¯•æ‰€æœ‰æœç´¢å¼•æ“"""
    print("\nğŸ”§ æµ‹è¯•æ‰€æœ‰æœç´¢å¼•æ“çŠ¶æ€...")
    
    if not crawler:
        print("âŒ çˆ¬è™«æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    try:
        test_results = crawler.test_all_engines()
        
        print("ğŸ“Š æœç´¢å¼•æ“æµ‹è¯•ç»“æœ:")
        for engine, result in test_results.items():
            status = "âœ…" if result['success'] else "âŒ"
            print(f"{status} {engine}: {result['result_count']} ä¸ªç»“æœ")
            
            if result['error']:
                print(f"    é”™è¯¯: {result['error']}")
            
            # æ˜¾ç¤ºæ ·ä¾‹ç»“æœ
            for i, sample in enumerate(result['sample_results']):
                print(f"    [{i+1}] {sample['title'][:40]}...")
        
        successful_engines = [e for e, r in test_results.items() if r['success']]
        print(f"\nğŸ“ˆ æˆåŠŸç‡: {len(successful_engines)}/{len(test_results)} ä¸ªå¼•æ“å¯ç”¨")
        
        return len(successful_engines) > 0
        
    except Exception as e:
        print(f"âŒ å¼•æ“æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_anti_crawler_detection():
    """æµ‹è¯•åçˆ¬è™«æ£€æµ‹"""
    print("\nğŸ”§ æµ‹è¯•åçˆ¬è™«æ£€æµ‹...")
    
    try:
        from keyword_crawler.crawler import SearchCrawler
        import requests
        
        crawler = SearchCrawler()
        
        # æ¨¡æ‹Ÿæ­£å¸¸å“åº”
        class MockResponse:
            def __init__(self, status_code, text):
                self.status_code = status_code
                self.text = text
        
        # æµ‹è¯•æ­£å¸¸å“åº”
        normal_response = MockResponse(200, "<html><body>æ­£å¸¸é¡µé¢</body></html>")
        assert not crawler._is_blocked(normal_response), "æ­£å¸¸å“åº”è¢«è¯¯åˆ¤ä¸ºæ‹¦æˆª"
        print("âœ… æ­£å¸¸å“åº”æ£€æµ‹æ­£ç¡®")
        
        # æµ‹è¯•æ‹¦æˆªå“åº”
        blocked_response = MockResponse(403, "<html><body>éªŒè¯ç  captcha</body></html>")
        assert crawler._is_blocked(blocked_response), "æ‹¦æˆªå“åº”æœªè¢«æ£€æµ‹åˆ°"
        print("âœ… æ‹¦æˆªå“åº”æ£€æµ‹æ­£ç¡®")
        
        return True
        
    except Exception as e:
        print(f"âŒ åçˆ¬è™«æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹çˆ¬è™«åŠŸèƒ½å…¨é¢æµ‹è¯•")
    print("=" * 50)
    
    setup_test_logging()
    
    # æµ‹è¯•æ­¥éª¤
    test_results = []
    
    # 1. æµ‹è¯•å¯¼å…¥
    test_results.append(("æ¨¡å—å¯¼å…¥", test_imports()))
    
    # 2. æµ‹è¯•ä¾èµ–
    test_results.append(("ä¾èµ–åŒ…æ£€æŸ¥", test_dependencies()))
    
    # 3. æµ‹è¯•åˆå§‹åŒ–
    crawler = test_crawler_initialization()
    test_results.append(("çˆ¬è™«åˆå§‹åŒ–", crawler is not None))
    
    # 4. æµ‹è¯•URLæ„å»º
    test_results.append(("URLæ„å»º", test_url_building()))
    
    # 5. æµ‹è¯•é€‰æ‹©å™¨
    test_results.append(("CSSé€‰æ‹©å™¨", test_selectors()))
    
    # 6. æµ‹è¯•åçˆ¬è™«æ£€æµ‹
    test_results.append(("åçˆ¬è™«æ£€æµ‹", test_anti_crawler_detection()))
    
    # 7. æµ‹è¯•æœç´¢åŠŸèƒ½ï¼ˆå¦‚æœä¾èµ–å¯ç”¨ï¼‰
    if test_results[1][1]:  # å¦‚æœä¾èµ–æ£€æŸ¥é€šè¿‡
        test_results.append(("å•å…³é”®è¯æœç´¢", test_search_single_keyword(crawler)))
        test_results.append(("æœç´¢å¼•æ“æµ‹è¯•", test_all_engines(crawler)))
    else:
        print("\nâš ï¸  è·³è¿‡æœç´¢åŠŸèƒ½æµ‹è¯•ï¼ˆç¼ºå°‘ä¾èµ–ï¼‰")
        test_results.append(("å•å…³é”®è¯æœç´¢", False))
        test_results.append(("æœç´¢å¼•æ“æµ‹è¯•", False))
    
    # æ±‡æ€»æµ‹è¯•ç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ±‡æ€»:")
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{status} {test_name}")
        if result:
            passed += 1
    
    print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çˆ¬è™«åŠŸèƒ½æ­£å¸¸")
        return 0
    elif passed >= total * 0.7:  # 70%ä»¥ä¸Šé€šè¿‡
        print("âš ï¸  å¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œçˆ¬è™«åŸºæœ¬å¯ç”¨")
        return 0
    else:
        print("ğŸš¨ å¤šé¡¹æµ‹è¯•å¤±è´¥ï¼Œçˆ¬è™«å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)